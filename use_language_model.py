from random import randint
from pickle import load
import numpy as np
from data_preparation import*
from keras.models import load_model
from keras_preprocessing.sequence import pad_sequences
from keras.utils import plot_model
import matplotlib.pyplot as plt

# load cleaned text sequences
in_filename = 'republic_sequences.txt'
doc = load_doc(in_filename)
lines = doc.split('\n') # list of sentences
seq_length = len(lines[0].split()) - 1 # equal to the no. of words in the first sentence minus one (right?)

# load the model
model = load_model('model.h5')
#print(model.summary())

# load the tokenizer
tokenizer = load(open('tokenizer.pkl', 'rb'))

def start_interactive_session(amount_of_predictions):
    """
    Allows the user to type words and returns the predictions until a keyboard interrupt occurs.

    Parameters
        amount_of_predictions: int
            The no. of words to be predicted.
    """
    print('\n\n\nInteractive session started\n===========================')
    while 1:
        print('Words predicted: ' + generate_seq(model, tokenizer, seq_length, input("Type a few words: "), amount_of_predictions) + '\n')

def generate_seq(model, tokenizer, seq_length, seed_text, n_words):
    """
    Returns a sequence of words generated by the model.

    Parameters
        model: Sequential
            The trained model that will perform the predictions.
        tokenizer: Tokenizer
            Will encode 'seed_text' into integers.
        seq_length: int
            The max no. of all sequences.
        seed_text: str
            A random sequence of 50 words.
        n_words: int
            The no. of words to predict.

    Returns
        :str
            A string made of the words predicted.
    """
    result = list()
    in_text = seed_text
    for _ in range(n_words):
        encoded = tokenizer.texts_to_sequences([in_text])[0] # encodes the text as integer
        encoded = pad_sequences([encoded], maxlen = seq_length, truncating = 'pre') # truncates sequences to a fixed length

        yhat = model.predict(encoded, verbose = 0) # numpy array of probabilities for each word
        index_of_word_with_max_probability = np.argmax(yhat, axis = 1) # https://stackoverflow.com/questions/68776790/model-predict-classes-is-deprecated-what-to-use-instead

        out_word = ''
        for word, index in tokenizer.word_index.items(): # map predicted word index to word
            if index == index_of_word_with_max_probability:
                out_word = word
                break
		# append to input
        in_text += ' ' + out_word
        result.append(out_word)
    return ' '.join(result) 

def get_sentence_completion(user_input, words_per_phrase):
    """
    Returns multiple words (i.e., a phrase) in response to user input.

    Parameters
        user_input: str
            A word or a phrase/sentence.
        words_per_phrase: int
            The number of words returned.
    
    Returns
        :str
            A string made of the words predicted.
    """
    generated = generate_seq(model, tokenizer, seq_length, user_input, words_per_phrase) # the number denotes the amount of words to be generated by the prediction
    print(f'Generated response to "{user_input}": {generated}')
    return generated

def get_most_probable_next_words(user_input, amount_of_words):
    """
    Returns a list of the most probable next words.

    Parameters
        user_input: str
            A word or a phrase/sentence.
        amount_of_words: int
            The length of the list of words to predict.

    Returns
        : list
            The list of the most probable next words.
    """
    most_probable_words = list()
    encoded = tokenizer.texts_to_sequences([user_input])[0] # encodes the text as integer
    encoded = pad_sequences([encoded], maxlen = seq_length, truncating = 'pre') # truncates sequences to a fixed length
    yhat = model.predict(encoded, verbose = 0) # list of probabilities for each word

    # sort, flatten, and slice the indices with the highest probabilities from the end
    indices_of_most_probable_words = np.argsort(yhat, axis = 1).flatten()[-amount_of_words:] # use 'flatten' because 'yhat' is a 2D array
    
    for word, index in tokenizer.word_index.items(): # map predicted word index to word
        if index in indices_of_most_probable_words:
            most_probable_words.append(word)
    return most_probable_words

# does not work
#plt.plot(model.history['acc'])
#plt.plot(model.history['val_acc'])
#plt.show()
